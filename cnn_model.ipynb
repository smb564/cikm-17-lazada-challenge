{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_path, clarity_path, con_path):\n",
    "    x = pd.read_csv(data_path, header=None, encoding='utf-8')[:-1]\n",
    "    clarity = pd.read_csv(clarity_path, header=None, encoding='utf-8')[:-1]\n",
    "    con = pd.read_csv(con_path, header=None, encoding='utf-8')[:-1]\n",
    "    return (x, clarity, con)\n",
    "\n",
    "x = pd.read_csv(\"training/data_train.csv\", header=None, encoding='utf-8')[:-1]\n",
    "clarity = pd.read_csv(\"training/clarity_train.labels\", header=None, encoding='utf-8')[:-1]\n",
    "con = pd.read_csv(\"training/conciseness_train.labels\", header=None, encoding='utf-8')[:-1]\n",
    "\n",
    "# train_x, train_clarity, train_con = process_data(\"training/data_train.csv\", \"training/clarity_train.labels\", \"training/conciseness_train.labels\")\n",
    "# val_x, val_clarity, val_con = \n",
    "\n",
    "clarity.columns = [\"clarity\"]\n",
    "con.columns = [\"conciseness\"]\n",
    "data = x.join(clarity).join(con)\n",
    "\n",
    "import re\n",
    "# data[6].apply(lambda x: re.sub(r\"<[0-9a-zA-Z/]*>\", \"\", x).strip())\n",
    "data.columns = [\"country\", \"id\", \"title\", \"cat_1\", \"cat_2\", \"cat_3\", \"description\", \"price\", \"product_type\", \"clarity\", \"conciseness\"]\n",
    "data.fillna(\"\", inplace=True)\n",
    "data.description = data.description.apply(lambda x: re.sub(r\"<[0-9a-zA-Z/]*>\", \"\", x).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 85975\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "word2id = {'<pad>':0}\n",
    "id2word = {0: '<pad>'}\n",
    "\n",
    "for title in data.title:\n",
    "    words = word_tokenize(title)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if not word in word2id:\n",
    "            word2id[word] = len(word2id)\n",
    "            id2word[len(word2id)-1] = word\n",
    "            \n",
    "for des in data.description:\n",
    "    words = word_tokenize(des)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if not word in word2id:\n",
    "            word2id[word] = len(word2id)\n",
    "            id2word[len(word2id)-1] = word\n",
    "\n",
    "word2id['<unk>'] = len(word2id)\n",
    "id2word[len(word2id)-1] = '<unk>'\n",
    "\n",
    "print \"vocab_size =\", len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"word2id.pickle\", \"wb\") as f:\n",
    "    pickle.dump(word2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category count = 245\n"
     ]
    }
   ],
   "source": [
    "cat2id = {}\n",
    "\n",
    "def update_car2id(series, cat2id):\n",
    "    for cat in series:\n",
    "        if not cat in cat2id:\n",
    "            cat2id[cat] = len(cat2id)\n",
    "    return cat2id\n",
    "\n",
    "cat2id = update_car2id(data.cat_1, cat2id)\n",
    "cat2id = update_car2id(data.cat_2, cat2id)\n",
    "cat2id = update_car2id(data.cat_3, cat2id)\n",
    "print \"category count =\", len(cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data.price = scaler.fit_transform(data[['price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.title = data.title.apply(lambda x: word_tokenize(x))\n",
    "data.description = data.description.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "data_count = len(data)\n",
    "\n",
    "title_length = max(map(len, data.title.tolist()))\n",
    "description_length = max(map(len, data.description.tolist()))\n",
    "# this becomes more than 4000, let's limit this\n",
    "description_length = 300\n",
    "\n",
    "x_title = np.zeros((data_count, title_length), dtype=np.int)\n",
    "x_desc = np.zeros((data_count, description_length), dtype=np.int)\n",
    "\n",
    "for i in xrange(data_count):\n",
    "    for j, token in enumerate(data.title[i]):\n",
    "        token = token.lower()\n",
    "        x_title[i][j] = word2id[token]\n",
    "    \n",
    "    for j, token in enumerate(data.description[i]):\n",
    "        token = token.lower()\n",
    "        x_desc[i][j] = word2id[token]\n",
    "        \n",
    "        # cut the description\n",
    "        if j==description_length-1:\n",
    "            break\n",
    "x_price = np.array(data.price.tolist(), np.float32)\n",
    "\n",
    "train_y1 = np.array(data.clarity.tolist(), dtype=np.float32)\n",
    "train_y2 = np.array(data.conciseness.tolist(), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class ClarityModel(nn.Module):\n",
    "    def __init__(self, emb_size, vocab_size, t_ks, d_ks, t_length, d_length):\n",
    "        super(ClarityModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, t_length, d_length)\n",
    "        self.title_cnn = nn.Conv2d(1, 1, (t_ks, emb_size))\n",
    "        self.desc_cnn = nn.Conv2d(1, 1, (d_ks, emb_size))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(t_length-(t_ks-1)+d_length-(d_ks-1)+1, 1) # stride is taken as 1 in this case\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.t_length = t_length\n",
    "        self.d_length = d_length\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.loss = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, title_idx, desc_idx, price):\n",
    "        # price should be (b_size,1)\n",
    "        batch_size = title_idx.shape[0]\n",
    "        t_emb = self.embedding(title_idx) #b_size, t_length, emb_size\n",
    "        d_emb = self.embedding(desc_idx) #b_size, d_length, emb_size\n",
    "        t_out = self.title_cnn(t_emb.view(-1,1,self.t_length,self.emb_size)).view(batch_size, -1)\n",
    "        d_out = self.desc_cnn(d_emb.view(-1,1,self.d_length,self.emb_size)).view(batch_size, -1)\n",
    "        t_out = self.relu(t_out)\n",
    "        d_out = self.relu(d_out)\n",
    "        out = torch.cat((t_out, d_out, price), dim=1)\n",
    "        out = self.linear(out)\n",
    "        final = self.sigmoid(out)\n",
    "        return final\n",
    "        \n",
    "    def get_loss(self, prediction, target):\n",
    "        return self.loss(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 50), (50, 80), (80, 102)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_batch_idx(data_count, batch_size, i = 0):\n",
    "    j = i + batch_size\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    while (i<data_count):\n",
    "        output.append((i,j))\n",
    "        i += batch_size\n",
    "        j += batch_size\n",
    "        \n",
    "        if j>data_count:\n",
    "            j = data_count\n",
    "    \n",
    "    return output\n",
    "\n",
    "generate_batch_idx(102, 30, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CNN_Text(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        V = args['embed_num']\n",
    "        D = args['embed_dim']\n",
    "        C = args['class_num']\n",
    "        Ci = 1\n",
    "        Co = args['kernel_num']\n",
    "        Ks = args['kernel_sizes']\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # (N, W, D)\n",
    "        \n",
    "#         if self.args.static:\n",
    "#             x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        logit = self.fc1(x)  # (N, C)\n",
    "        return F.sigmoid(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2000, Batch count:15\n",
      "Training loss at epoch 1: 39.62837255\n",
      "Training set score at epoch 1: 0.5690676096\n",
      "Validation score at epoch 1: 0.540638334962\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 2: 29.6202298403\n",
      "Training set score at epoch 2: 0.528727451185\n",
      "Validation score at epoch 2: 0.504024833161\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 3: 18.502684772\n",
      "Training set score at epoch 3: 0.48364497401\n",
      "Validation score at epoch 3: 0.483924226088\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 4: 12.9414686561\n",
      "Training set score at epoch 4: 0.465933511945\n",
      "Validation score at epoch 4: 0.475922996279\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 5: 11.1642659903\n",
      "Training set score at epoch 5: 0.467059622197\n",
      "Validation score at epoch 5: 0.46615489\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 6: 10.2210943103\n",
      "Training set score at epoch 6: 0.464437077062\n",
      "Validation score at epoch 6: 0.463026592092\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 7: 9.86550658941\n",
      "Training set score at epoch 7: 0.463902441518\n",
      "Validation score at epoch 7: 0.461983566117\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 8: 9.57638251781\n",
      "Training set score at epoch 8: 0.462927293899\n",
      "Validation score at epoch 8: 0.46068511442\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 9: 9.42303442955\n",
      "Training set score at epoch 9: 0.463167332137\n",
      "Validation score at epoch 9: 0.462036455602\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 10: 9.36616754532\n",
      "Training set score at epoch 10: 0.463433912483\n",
      "Validation score at epoch 10: 0.461501828434\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 11: 9.34344267845\n",
      "Training set score at epoch 11: 0.46327179311\n",
      "Validation score at epoch 11: 0.460845374236\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 12: 9.29179781675\n",
      "Training set score at epoch 12: 0.462898418204\n",
      "Validation score at epoch 12: 0.46117915594\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 13: 9.29405373335\n",
      "Training set score at epoch 13: 0.462902946755\n",
      "Validation score at epoch 13: 0.460737147831\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 14: 9.28494745493\n",
      "Training set score at epoch 14: 0.462534657499\n",
      "Validation score at epoch 14: 0.461820384548\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 15: 9.28818428516\n",
      "Training set score at epoch 15: 0.46261319893\n",
      "Validation score at epoch 15: 0.460980665146\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 16: 9.28993278742\n",
      "Training set score at epoch 16: 0.462355145305\n",
      "Validation score at epoch 16: 0.461473805205\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 17: 209.591065645\n",
      "Training set score at epoch 17: 0.743507136799\n",
      "Validation score at epoch 17: 0.829671203728\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 18: 282.313598633\n",
      "Training set score at epoch 18: 0.825569974527\n",
      "Validation score at epoch 18: 0.829671186098\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 19: 282.312639236\n",
      "Training set score at epoch 19: 0.825553664991\n",
      "Validation score at epoch 19: 0.829844614988\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 20: 282.364107132\n",
      "Training set score at epoch 20: 0.825693029405\n",
      "Validation score at epoch 20: 0.829688637021\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 21: 282.319747925\n",
      "Training set score at epoch 21: 0.825621468359\n",
      "Validation score at epoch 21: 0.829700914854\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 22: 282.316841125\n",
      "Training set score at epoch 22: 0.825617595895\n",
      "Validation score at epoch 22: 0.829712758107\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 23: 282.318029404\n",
      "Training set score at epoch 23: 0.825631672435\n",
      "Validation score at epoch 23: 0.829722417728\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 24: 282.3189888\n",
      "Training set score at epoch 24: 0.825642319715\n",
      "Validation score at epoch 24: 0.829729122516\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 25: 282.31962204\n",
      "Training set score at epoch 25: 0.825648831689\n",
      "Validation score at epoch 25: 0.829732527813\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 26: 282.319812775\n",
      "Training set score at epoch 26: 0.825650910254\n",
      "Validation score at epoch 26: 0.829732525882\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 27: 282.319572449\n",
      "Training set score at epoch 27: 0.825648476524\n",
      "Validation score at epoch 27: 0.829729153222\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 28: 282.318920135\n",
      "Training set score at epoch 28: 0.825641609362\n",
      "Validation score at epoch 28: 0.829722600739\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 29: 282.317930222\n",
      "Training set score at epoch 29: 0.825630613325\n",
      "Validation score at epoch 29: 0.829713325339\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 30: 282.316720963\n",
      "Training set score at epoch 30: 0.825616213715\n",
      "Validation score at epoch 30: 0.829702253514\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 31: 282.31570816\n",
      "Training set score at epoch 31: 0.825604083641\n",
      "Validation score at epoch 31: 0.829691035086\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 32: 282.320108414\n",
      "Training set score at epoch 32: 0.825602433123\n",
      "Validation score at epoch 32: 0.829682171376\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 33: 282.315748215\n",
      "Training set score at epoch 33: 0.825589449309\n",
      "Validation score at epoch 33: 0.829678792305\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 34: 263.673956871\n",
      "Training set score at epoch 34: 0.815051129821\n",
      "Validation score at epoch 34: 0.460316102402\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 35: 9.30137711763\n",
      "Training set score at epoch 35: 0.46283810978\n",
      "Validation score at epoch 35: 0.460062951608\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 36: 9.30055332184\n",
      "Training set score at epoch 36: 0.462497467662\n",
      "Validation score at epoch 36: 0.460349383634\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 37: 9.33502262831\n",
      "Training set score at epoch 37: 0.46278542288\n",
      "Validation score at epoch 37: 0.460281880342\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 38: 9.35963374376\n",
      "Training set score at epoch 38: 0.462945819889\n",
      "Validation score at epoch 38: 0.460723487843\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 39: 9.40763950348\n",
      "Training set score at epoch 39: 0.463385579366\n",
      "Validation score at epoch 39: 0.461445662664\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 40: 34.2673227787\n",
      "Training set score at epoch 40: 0.504369169499\n",
      "Validation score at epoch 40: 0.747145672883\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 41: 175.004874229\n",
      "Training set score at epoch 41: 0.659101703086\n",
      "Validation score at epoch 41: 0.552263959088\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 42: 130.456777573\n",
      "Training set score at epoch 42: 0.561686646883\n",
      "Validation score at epoch 42: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 43: 130.722166061\n",
      "Training set score at epoch 43: 0.562080545109\n",
      "Validation score at epoch 43: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 44: 130.722166061\n",
      "Training set score at epoch 44: 0.562080545109\n",
      "Validation score at epoch 44: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 45: 130.722166061\n",
      "Training set score at epoch 45: 0.562080545109\n",
      "Validation score at epoch 45: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 46: 130.722166061\n",
      "Training set score at epoch 46: 0.562080545109\n",
      "Validation score at epoch 46: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 47: 130.722166061\n",
      "Training set score at epoch 47: 0.562080545109\n",
      "Validation score at epoch 47: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 48: 130.722166061\n",
      "Training set score at epoch 48: 0.562080545109\n",
      "Validation score at epoch 48: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 49: 130.722166061\n",
      "Training set score at epoch 49: 0.562080545109\n",
      "Validation score at epoch 49: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 50: 130.722166061\n",
      "Training set score at epoch 50: 0.562080545109\n",
      "Validation score at epoch 50: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 51: 130.722166061\n",
      "Training set score at epoch 51: 0.562080545109\n",
      "Validation score at epoch 51: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 52: 130.722166061\n",
      "Training set score at epoch 52: 0.562080545109\n",
      "Validation score at epoch 52: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 53: 130.722166061\n",
      "Training set score at epoch 53: 0.562080545109\n",
      "Validation score at epoch 53: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 54: 130.722166061\n",
      "Training set score at epoch 54: 0.562080545109\n",
      "Validation score at epoch 54: 0.556322102189\n",
      "------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 55: 130.722166061\n",
      "Training set score at epoch 55: 0.562080545109\n",
      "Validation score at epoch 55: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 56: 130.722166061\n",
      "Training set score at epoch 56: 0.562080545109\n",
      "Validation score at epoch 56: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 57: 130.722166061\n",
      "Training set score at epoch 57: 0.562080545109\n",
      "Validation score at epoch 57: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 58: 130.722166061\n",
      "Training set score at epoch 58: 0.562080545109\n",
      "Validation score at epoch 58: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 59: 130.722166061\n",
      "Training set score at epoch 59: 0.562080545109\n",
      "Validation score at epoch 59: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 60: 130.722166061\n",
      "Training set score at epoch 60: 0.562080545109\n",
      "Validation score at epoch 60: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 61: 130.722166061\n",
      "Training set score at epoch 61: 0.562080545109\n",
      "Validation score at epoch 61: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 62: 130.722166061\n",
      "Training set score at epoch 62: 0.562080545109\n",
      "Validation score at epoch 62: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 63: 130.722166061\n",
      "Training set score at epoch 63: 0.562080545109\n",
      "Validation score at epoch 63: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 64: 130.722166061\n",
      "Training set score at epoch 64: 0.562080545109\n",
      "Validation score at epoch 64: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 65: 130.722166061\n",
      "Training set score at epoch 65: 0.562080545109\n",
      "Validation score at epoch 65: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 66: 130.722166061\n",
      "Training set score at epoch 66: 0.562080545109\n",
      "Validation score at epoch 66: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 67: 130.722166061\n",
      "Training set score at epoch 67: 0.562080545109\n",
      "Validation score at epoch 67: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 68: 130.722166061\n",
      "Training set score at epoch 68: 0.562080545109\n",
      "Validation score at epoch 68: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 69: 130.722166061\n",
      "Training set score at epoch 69: 0.562080545109\n",
      "Validation score at epoch 69: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 70: 130.722166061\n",
      "Training set score at epoch 70: 0.562080545109\n",
      "Validation score at epoch 70: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 71: 130.722166061\n",
      "Training set score at epoch 71: 0.562080545109\n",
      "Validation score at epoch 71: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 72: 130.722166061\n",
      "Training set score at epoch 72: 0.562080545109\n",
      "Validation score at epoch 72: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 73: 130.722166061\n",
      "Training set score at epoch 73: 0.562080545109\n",
      "Validation score at epoch 73: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 74: 130.722166061\n",
      "Training set score at epoch 74: 0.562080545109\n",
      "Validation score at epoch 74: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 75: 130.722166061\n",
      "Training set score at epoch 75: 0.562080545109\n",
      "Validation score at epoch 75: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 76: 130.722166061\n",
      "Training set score at epoch 76: 0.562080545109\n",
      "Validation score at epoch 76: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 77: 130.722166061\n",
      "Training set score at epoch 77: 0.562080545109\n",
      "Validation score at epoch 77: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 78: 130.722166061\n",
      "Training set score at epoch 78: 0.562080545109\n",
      "Validation score at epoch 78: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 79: 130.722166061\n",
      "Training set score at epoch 79: 0.562080545109\n",
      "Validation score at epoch 79: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 80: 130.722166061\n",
      "Training set score at epoch 80: 0.562080545109\n",
      "Validation score at epoch 80: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 81: 130.722166061\n",
      "Training set score at epoch 81: 0.562080545109\n",
      "Validation score at epoch 81: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 82: 130.722166061\n",
      "Training set score at epoch 82: 0.562080545109\n",
      "Validation score at epoch 82: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 83: 130.722166061\n",
      "Training set score at epoch 83: 0.562080545109\n",
      "Validation score at epoch 83: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 84: 130.722166061\n",
      "Training set score at epoch 84: 0.562080545109\n",
      "Validation score at epoch 84: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 85: 130.722166061\n",
      "Training set score at epoch 85: 0.562080545109\n",
      "Validation score at epoch 85: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 86: 130.722166061\n",
      "Training set score at epoch 86: 0.562080545109\n",
      "Validation score at epoch 86: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 87: 130.722166061\n",
      "Training set score at epoch 87: 0.562080545109\n",
      "Validation score at epoch 87: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 88: 130.722166061\n",
      "Training set score at epoch 88: 0.562080545109\n",
      "Validation score at epoch 88: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 89: 130.722166061\n",
      "Training set score at epoch 89: 0.562080545109\n",
      "Validation score at epoch 89: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 90: 130.722166061\n",
      "Training set score at epoch 90: 0.562080545109\n",
      "Validation score at epoch 90: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 91: 130.722166061\n",
      "Training set score at epoch 91: 0.562080545109\n",
      "Validation score at epoch 91: 0.556322102189\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 92: 130.750471115\n",
      "Training set score at epoch 92: 0.562172480973\n",
      "Validation score at epoch 92: 0.556569741685\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 93: 131.314884186\n",
      "Training set score at epoch 93: 0.56333570325\n",
      "Validation score at epoch 93: 0.55706469042\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 94: 131.42540741\n",
      "Training set score at epoch 94: 0.563580286222\n",
      "Validation score at epoch 94: 0.557188358906\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 95: 131.453037262\n",
      "Training set score at epoch 95: 0.563641415382\n",
      "Validation score at epoch 95: 0.55731199995\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 96: 131.425406456\n",
      "Training set score at epoch 96: 0.563580286222\n",
      "Validation score at epoch 96: 0.55731199995\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 97: 131.289273262\n",
      "Training set score at epoch 97: 0.563366281929\n",
      "Validation score at epoch 97: 0.556198241093\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 98: 134.843881607\n",
      "Training set score at epoch 98: 0.570174580757\n",
      "Validation score at epoch 98: 0.597754283357\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 99: 209.964625359\n",
      "Training set score at epoch 99: 0.707240638431\n",
      "Validation score at epoch 99: 0.828302664879\n",
      "------------------\n",
      "\n",
      "Training loss at epoch 100: 281.730831146\n",
      "Training set score at epoch 100: 0.82443308667\n",
      "Validation score at epoch 100: 0.828225908063\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# model = CNN_Text({'embed_num':len(word2id), 'embed_dim':50, 'class_num':1, 'kernel_num':3, 'kernel_sizes':(3,4,5), 'dropout':0.5})\n",
    "# loss_fn = nn.BCELoss()\n",
    "\n",
    "model = ClarityModel(50, len(word2id), 3, 3, title_length, description_length)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.17, weight_decay=1e-4)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 2000\n",
    "\n",
    "## since I don't have access to validation or test data, create a small devset using train set 80:20 split\n",
    "data_count = int(len(train_y1) * 0.8)\n",
    "\n",
    "batch_idx_list = generate_batch_idx(data_count, batch_size)\n",
    "val_idx_list = generate_batch_idx(len(train_y1), batch_size, data_count)\n",
    "\n",
    "print \"Batch size: {}, Batch count:{}\".format(batch_size, len(batch_idx_list))\n",
    "for epoch in xrange(epochs):\n",
    "    loss_val = 0\n",
    "    loss_mse = 0\n",
    "    \n",
    "    for i,j in batch_idx_list:\n",
    "        xt_batch = torch.tensor(x_title[i:j], dtype=torch.long)\n",
    "        xd_batch = torch.tensor(x_desc[i:j], dtype=torch.long)\n",
    "        xp_batch = torch.tensor(x_price[i:j]).view(-1, 1)\n",
    "        \n",
    "        \n",
    "        prediction = model(xt_batch, xd_batch, xp_batch)\n",
    "        target = torch.tensor(train_y2[i:j]).view(-1,1)\n",
    "        loss = model.get_loss(prediction, target)\n",
    "        loss_val += loss.tolist()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ##############\n",
    "#         prediction = model(torch.cat((xt_batch, xd_batch), dim=1))\n",
    "#         target = torch.tensor(train_y2[i:j]).view(-1,1)\n",
    "#         loss = loss_fn(prediction, target)\n",
    "#         loss_val += loss.tolist()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        ##############\n",
    "        \n",
    "        \n",
    "        mse = mean_squared_error(prediction.view(-1).tolist(), target.tolist()) * (j-i)\n",
    "        loss_mse += mse\n",
    "\n",
    "    loss_mse /= data_count\n",
    "    loss_mse = loss_mse ** 0.5\n",
    "    \n",
    "        \n",
    "    print \"Training loss at epoch {}: {}\".format(epoch+1, loss_val)\n",
    "    print \"Training set score at epoch {}: {}\".format(epoch+1, loss_mse)\n",
    "    \n",
    "    loss_val = 0\n",
    "    \n",
    "    for i,j in val_idx_list:\n",
    "        t_val = torch.tensor(x_title[i:j], dtype=torch.long)\n",
    "        d_val = torch.tensor(x_desc[i:j], dtype=torch.long)\n",
    "        p_val = torch.tensor(x_price[i:j]).view(-1, 1)\n",
    "\n",
    "        prediction = model(t_val, d_val, p_val)\n",
    "#         prediction = model(torch.cat((t_val, d_val), dim=1))\n",
    "        target = train_y2[i:j]\n",
    "        loss = mean_squared_error(prediction.view(-1).tolist(), target.tolist()) * (j-i)\n",
    "        loss_val += loss\n",
    "    \n",
    "    loss_val /= (len(train_y1) - data_count)\n",
    "    loss_val = loss_val ** 0.5\n",
    "    print \"Validation score at epoch {}: {}\".format(epoch+1, loss_val)\n",
    "    print \"------------------\\n\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
